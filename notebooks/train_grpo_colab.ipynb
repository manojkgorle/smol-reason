{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Train a Reasoning LLM with GRPO (R1-Zero Replication)\n\nThis notebook trains **Qwen2.5-3B** using pure RL (GRPO) to develop reasoning capabilities,\nreplicating the DeepSeek-R1-Zero experiment.\n\n**Primary target:** H100 80GB (~6h, ~$18 on Jarvis Labs)\n**Also supports:** A100 80GB (falls back to 1.5B config), A100 40GB, and smaller GPUs."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers trl accelerate datasets math-verify wandb tensorboard pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repo (or upload files)\n",
    "import os\n",
    "REPO_URL = \"https://github.com/manojkgorle/smol-reason\"\n",
    "PROJECT_DIR = \"/content/smol-reason\"\n",
    "\n",
    "if REPO_URL and not os.path.exists(PROJECT_DIR):\n",
    "    !git clone {REPO_URL} {PROJECT_DIR}\n",
    "elif not os.path.exists(PROJECT_DIR):\n",
    "    print(\"Please upload the reson-llm project files or set REPO_URL\")\n",
    "\n",
    "os.chdir(PROJECT_DIR)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# GPU check and memory-based config selection\nimport torch\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"No GPU detected! This notebook requires a GPU.\")\n\ngpu_name = torch.cuda.get_device_name(0)\ngpu_mem_gb = torch.cuda.get_device_properties(0).total_mem / 1e9\nprint(f\"GPU: {gpu_name}\")\nprint(f\"Memory: {gpu_mem_gb:.1f} GB\")\n\n# Auto-select config based on GPU type and memory\n# H100 80GB → 3B model (best results, full utilization of H100)\n# A100 80GB → 1.5B model with G=16 (full config)\n# A100 40GB → 1.5B model with G=8 (reduced)\n# Other     → 1.5B model with minimal config\nis_h100 = \"H100\" in gpu_name or \"H200\" in gpu_name\n\nif is_h100 and gpu_mem_gb >= 70:\n    GPU_TIER = \"h100_80gb\"\n    CONFIG_FILE = \"configs/grpo_qwen2.5_3b_h100.yaml\"\n    print(\"Config: Qwen2.5-3B on H100 (G=12, max_completion=1536)\")\nelif gpu_mem_gb >= 70:\n    GPU_TIER = \"a100_80gb\"\n    CONFIG_FILE = \"configs/grpo_qwen2.5_1.5b.yaml\"\n    print(\"Config: Qwen2.5-1.5B full (G=16, max_completion=2048)\")\nelif gpu_mem_gb >= 35:\n    GPU_TIER = \"40gb\"\n    CONFIG_FILE = \"configs/grpo_qwen2.5_1.5b.yaml\"\n    print(\"Config: Qwen2.5-1.5B reduced (G=8, max_completion=1024)\")\nelse:\n    GPU_TIER = \"small\"\n    CONFIG_FILE = \"configs/grpo_qwen2.5_1.5b.yaml\"\n    print(\"WARNING: Limited GPU memory. Using minimal config.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B login\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive mount for checkpoint persistence\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "DRIVE_CHECKPOINT_DIR = \"/content/drive/MyDrive/smol-reason-checkpoints\"\n",
    "os.makedirs(DRIVE_CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"Checkpoints will be saved to: {DRIVE_CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import yaml\n\n# Load the auto-selected config\nprint(f\"Loading config: {CONFIG_FILE}\")\nwith open(CONFIG_FILE) as f:\n    config = yaml.safe_load(f)\n\n# Override output_dir to use Google Drive\nconfig[\"output_dir\"] = DRIVE_CHECKPOINT_DIR\n\n# Apply GPU-tier-specific overrides for non-H100/non-80GB GPUs\nif GPU_TIER == \"40gb\":\n    config[\"num_generations\"] = 8\n    config[\"max_completion_length\"] = 1024\n    config[\"per_device_train_batch_size\"] = 1\n    print(\"Adjusted config for 40GB GPU\")\nelif GPU_TIER == \"small\":\n    config[\"num_generations\"] = 4\n    config[\"max_completion_length\"] = 512\n    config[\"per_device_train_batch_size\"] = 1\n    config[\"gradient_accumulation_steps\"] = 4\n    print(\"Adjusted config for small GPU\")\n\n# Write adjusted config\nRUNTIME_CONFIG = \"/content/runtime_config.yaml\"\nwith open(RUNTIME_CONFIG, \"w\") as f:\n    yaml.dump(config, f, default_flow_style=False)\n\nprint(f\"\\nModel: {config['model_name_or_path']}\")\nprint(f\"GPU tier: {GPU_TIER}\")\nprint(f\"\\nTraining config:\")\nfor k, v in config.items():\n    print(f\"  {k}: {v}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing checkpoint to resume from\n",
    "import glob\n",
    "\n",
    "checkpoints = sorted(glob.glob(f\"{DRIVE_CHECKPOINT_DIR}/checkpoint-*\"))\n",
    "resume_arg = \"\"\n",
    "if checkpoints:\n",
    "    latest = checkpoints[-1]\n",
    "    print(f\"Found checkpoint: {latest}\")\n",
    "    resume_arg = f\"--resume_from_checkpoint {latest}\"\n",
    "else:\n",
    "    print(\"No checkpoint found, starting fresh.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch training\n",
    "!python src/train_grpo.py --config {RUNTIME_CONFIG} {resume_arg}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monitor (TensorBoard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {DRIVE_CHECKPOINT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Quick eval on 200 samples\n!python src/evaluate.py \\\n    --model_path {DRIVE_CHECKPOINT_DIR} \\\n    --num_samples 200 \\\n    --output_dir eval_results\n\nprint(f\"\\nModel evaluated: {config['model_name_or_path']} (trained)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Baseline comparison (raw base model, before RL training)\nBASE_MODEL = config[\"model_name_or_path\"]\nprint(f\"Evaluating baseline: {BASE_MODEL}\")\n!python src/evaluate.py \\\n    --model_path {BASE_MODEL} \\\n    --num_samples 200 \\\n    --output_dir eval_results_baseline"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "import json\n",
    "\n",
    "with open(\"eval_results/summary.json\") as f:\n",
    "    trained = json.load(f)\n",
    "with open(\"eval_results_baseline/summary.json\") as f:\n",
    "    baseline = json.load(f)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<30} {'Baseline':>12} {'Trained':>12}\")\n",
    "print(\"=\" * 60)\n",
    "for dataset in [\"gsm8k\", \"math\"]:\n",
    "    if dataset in trained and dataset in baseline:\n",
    "        b = baseline[dataset]\n",
    "        t = trained[dataset]\n",
    "        print(f\"{dataset.upper()} accuracy:{'':>13} {b['accuracy']:>11.1%} {t['accuracy']:>11.1%}\")\n",
    "        print(f\"{dataset.upper()} format compliance:{'':>4} {b['format_compliance']:>11.1%} {t['format_compliance']:>11.1%}\")\n",
    "        print(f\"{dataset.upper()} avg think tokens:{'':>5} {b['avg_think_tokens']:>11.0f} {t['avg_think_tokens']:>11.0f}\")\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Push to Hub (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Uncomment and set your HF username to push\n# from huggingface_hub import login\n# login()\n\n# HF_USERNAME = \"your-username\"\n# MODEL_TAG = config[\"model_name_or_path\"].split(\"/\")[-1].lower()\n# REPO_NAME = f\"{HF_USERNAME}/{MODEL_TAG}-r1zero-grpo\"\n# from transformers import AutoModelForCausalLM, AutoTokenizer\n# model = AutoModelForCausalLM.from_pretrained(DRIVE_CHECKPOINT_DIR, torch_dtype=torch.bfloat16)\n# tokenizer = AutoTokenizer.from_pretrained(DRIVE_CHECKPOINT_DIR)\n# model.push_to_hub(REPO_NAME)\n# tokenizer.push_to_hub(REPO_NAME)\n# print(f\"Pushed to: https://huggingface.co/{REPO_NAME}\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}