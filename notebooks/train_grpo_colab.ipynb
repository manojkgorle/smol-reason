{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Train a Reasoning LLM with GRPO (R1-Zero Replication)\n\nThis notebook trains **Qwen2.5-3B** using pure RL (GRPO) to develop reasoning capabilities,\nreplicating the DeepSeek-R1-Zero experiment.\n\n**Supported setups** (auto-detected):\n| Setup | Config | Time | Cost |\n|-------|--------|------|------|\n| 1× H200 141GB | 3B, G=16, batch=2 | ~5h | — |\n| 3× A100 80GB (Jarvis Labs) | 3B, G=12, data-parallel | ~4.3h | ~$17 |\n| 2× A100 80GB | 3B, G=12, data-parallel | ~5.5h | ~$14 |\n| 1× H100 80GB | 3B, G=12 | ~6h | ~$18 |\n| 1× A100 80GB | 1.5B, G=16 | ~8h | ~$10 |\n| 1× A100 40GB | 1.5B, G=8 | ~5.5h | ~$7 |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers trl accelerate datasets math-verify wandb tensorboard pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Platform + clone repo\nimport os\n\nPLATFORM = \"jarvis\"  # Change to \"colab\" if running on Google Colab\nREPO_URL = \"https://github.com/manojkgorle/smol-reason\"\n\nif PLATFORM == \"colab\":\n    PROJECT_DIR = \"/content/smol-reason\"\nelse:\n    # Jarvis Labs — use home directory\n    PROJECT_DIR = os.path.expanduser(\"~/smol-reason\")\n\nif REPO_URL and not os.path.exists(PROJECT_DIR):\n    !git clone {REPO_URL} {PROJECT_DIR}\nelif not os.path.exists(PROJECT_DIR):\n    print(\"Please upload the reson-llm project files or set REPO_URL\")\n\nos.chdir(PROJECT_DIR)\nprint(f\"Platform: {PLATFORM}\")\nprint(f\"Working directory: {os.getcwd()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# GPU check — detect count, type, and memory to auto-select config\nimport torch\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"No GPU detected! This notebook requires a GPU.\")\n\nNUM_GPUS = torch.cuda.device_count()\ngpu_name = torch.cuda.get_device_name(0)\ngpu_mem_gb = torch.cuda.get_device_properties(0).total_mem / 1e9\n\nprint(f\"GPUs found: {NUM_GPUS}× {gpu_name}\")\nprint(f\"Memory per GPU: {gpu_mem_gb:.1f} GB\")\nprint(f\"Total GPU memory: {NUM_GPUS * gpu_mem_gb:.1f} GB\")\n\n# Auto-select config:\n#   1× H200 141GB → 3B model, G=16, batch=2 (best single-GPU config)\n#   3× A100 80GB  → 3B model, data-parallel\n#   2× A100 80GB  → 3B model, data-parallel\n#   1× H100 80GB  → 3B model, single GPU\n#   1× A100 80GB  → 1.5B model, G=16\n#   1× A100 40GB  → 1.5B model, G=8\n#   Other         → 1.5B model, minimal\nis_h200 = \"H200\" in gpu_name\nis_h100 = \"H100\" in gpu_name\nis_a100_80 = \"A100\" in gpu_name and gpu_mem_gb >= 70\n\nif is_h200 and gpu_mem_gb >= 130:\n    GPU_TIER = \"h200_141gb\"\n    CONFIG_FILE = \"configs/grpo_qwen2.5_3b_h200.yaml\"\n    print(\"\\nConfig: Qwen2.5-3B on H200 (G=16, batch=2, max_completion=2048)\")\nelif NUM_GPUS >= 3 and is_a100_80:\n    GPU_TIER = \"3xa100_80gb\"\n    CONFIG_FILE = \"configs/grpo_qwen2.5_3b_a100x3.yaml\"\n    print(f\"\\nConfig: Qwen2.5-3B on {NUM_GPUS}× A100 80GB (data-parallel, G=12)\")\nelif NUM_GPUS >= 2 and is_a100_80:\n    GPU_TIER = \"2xa100_80gb\"\n    CONFIG_FILE = \"configs/grpo_qwen2.5_3b_a100x2.yaml\"\n    print(f\"\\nConfig: Qwen2.5-3B on {NUM_GPUS}× A100 80GB (data-parallel, G=12)\")\nelif (is_h100 or is_h200) and gpu_mem_gb >= 70:\n    GPU_TIER = \"h100_80gb\"\n    CONFIG_FILE = \"configs/grpo_qwen2.5_3b_h100.yaml\"\n    print(\"\\nConfig: Qwen2.5-3B on H100 (G=12, max_completion=1536)\")\nelif gpu_mem_gb >= 70:\n    GPU_TIER = \"a100_80gb\"\n    CONFIG_FILE = \"configs/grpo_qwen2.5_1.5b.yaml\"\n    print(\"\\nConfig: Qwen2.5-1.5B full (G=16, max_completion=2048)\")\nelif gpu_mem_gb >= 35:\n    GPU_TIER = \"40gb\"\n    CONFIG_FILE = \"configs/grpo_qwen2.5_1.5b.yaml\"\n    print(\"\\nConfig: Qwen2.5-1.5B reduced (G=8, max_completion=1024)\")\nelse:\n    GPU_TIER = \"small\"\n    CONFIG_FILE = \"configs/grpo_qwen2.5_1.5b.yaml\"\n    print(\"\\nWARNING: Limited GPU memory. Using minimal config.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B login\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Checkpoint storage setup\n#\n# Jarvis Labs: persistent local storage — checkpoints survive session stop/restart.\n# Google Colab: mount Google Drive for persistence across session disconnects.\n\nif PLATFORM == \"colab\":\n    from google.colab import drive\n    drive.mount(\"/content/drive\")\n    CHECKPOINT_DIR = \"/content/drive/MyDrive/reson-llm-checkpoints\"\nelse:\n    # Jarvis Labs / generic GPU server — local storage persists\n    CHECKPOINT_DIR = \"outputs/grpo-qwen2.5-3b\"\n\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\nprint(f\"Checkpoints: {CHECKPOINT_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import yaml\n\n# Load the auto-selected config\nprint(f\"Loading config: {CONFIG_FILE}\")\nwith open(CONFIG_FILE) as f:\n    config = yaml.safe_load(f)\n\n# Override output_dir to use our checkpoint directory\nconfig[\"output_dir\"] = CHECKPOINT_DIR\n\n# Apply GPU-tier-specific overrides for smaller GPUs\nif GPU_TIER == \"40gb\":\n    config[\"num_generations\"] = 8\n    config[\"max_completion_length\"] = 1024\n    config[\"per_device_train_batch_size\"] = 1\n    print(\"Adjusted config for 40GB GPU\")\nelif GPU_TIER == \"small\":\n    config[\"num_generations\"] = 4\n    config[\"max_completion_length\"] = 512\n    config[\"per_device_train_batch_size\"] = 1\n    config[\"gradient_accumulation_steps\"] = 4\n    print(\"Adjusted config for small GPU\")\n\n# Write adjusted config\nRUNTIME_CONFIG = os.path.join(PROJECT_DIR, \"runtime_config.yaml\")\nwith open(RUNTIME_CONFIG, \"w\") as f:\n    yaml.dump(config, f, default_flow_style=False)\n\nprint(f\"\\nModel: {config['model_name_or_path']}\")\nprint(f\"GPU tier: {GPU_TIER} ({NUM_GPUS}× GPU)\")\nprint(f\"\\nTraining config:\")\nfor k, v in config.items():\n    print(f\"  {k}: {v}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check for existing checkpoint to resume from\nimport glob\n\ncheckpoints = sorted(glob.glob(f\"{CHECKPOINT_DIR}/checkpoint-*\"))\nresume_arg = \"\"\nif checkpoints:\n    latest = checkpoints[-1]\n    print(f\"Found checkpoint: {latest}\")\n    resume_arg = f\"--resume_from_checkpoint {latest}\"\nelse:\n    print(\"No checkpoint found, starting fresh.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Launch training\n# Multi-GPU: use accelerate launch with data parallelism\n# Single-GPU: run directly with python\n\nif NUM_GPUS > 1:\n    print(f\"Launching distributed training on {NUM_GPUS} GPUs...\")\n    !accelerate launch --num_processes {NUM_GPUS} src/train_grpo.py \\\n        --config {RUNTIME_CONFIG} {resume_arg}\nelse:\n    print(\"Launching single-GPU training...\")\n    !python src/train_grpo.py --config {RUNTIME_CONFIG} {resume_arg}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monitor (TensorBoard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%load_ext tensorboard\n%tensorboard --logdir {CHECKPOINT_DIR}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Quick eval on 200 samples\n!python src/evaluate.py \\\n    --model_path {CHECKPOINT_DIR} \\\n    --num_samples 200 \\\n    --output_dir eval_results\n\nprint(f\"\\nModel evaluated: {config['model_name_or_path']} (trained)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Baseline comparison (raw base model, before RL training)\nBASE_MODEL = config[\"model_name_or_path\"]\nprint(f\"Evaluating baseline: {BASE_MODEL}\")\n!python src/evaluate.py \\\n    --model_path {BASE_MODEL} \\\n    --num_samples 200 \\\n    --output_dir eval_results_baseline"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "import json\n",
    "\n",
    "with open(\"eval_results/summary.json\") as f:\n",
    "    trained = json.load(f)\n",
    "with open(\"eval_results_baseline/summary.json\") as f:\n",
    "    baseline = json.load(f)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<30} {'Baseline':>12} {'Trained':>12}\")\n",
    "print(\"=\" * 60)\n",
    "for dataset in [\"gsm8k\", \"math\"]:\n",
    "    if dataset in trained and dataset in baseline:\n",
    "        b = baseline[dataset]\n",
    "        t = trained[dataset]\n",
    "        print(f\"{dataset.upper()} accuracy:{'':>13} {b['accuracy']:>11.1%} {t['accuracy']:>11.1%}\")\n",
    "        print(f\"{dataset.upper()} format compliance:{'':>4} {b['format_compliance']:>11.1%} {t['format_compliance']:>11.1%}\")\n",
    "        print(f\"{dataset.upper()} avg think tokens:{'':>5} {b['avg_think_tokens']:>11.0f} {t['avg_think_tokens']:>11.0f}\")\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Push to Hub (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Uncomment and set your HF username to push\n# from huggingface_hub import login\n# login()\n\n# HF_USERNAME = \"your-username\"\n# MODEL_TAG = config[\"model_name_or_path\"].split(\"/\")[-1].lower()\n# REPO_NAME = f\"{HF_USERNAME}/{MODEL_TAG}-r1zero-grpo\"\n# from transformers import AutoModelForCausalLM, AutoTokenizer\n# model = AutoModelForCausalLM.from_pretrained(CHECKPOINT_DIR, torch_dtype=torch.bfloat16)\n# tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_DIR)\n# model.push_to_hub(REPO_NAME)\n# tokenizer.push_to_hub(REPO_NAME)\n# print(f\"Pushed to: https://huggingface.co/{REPO_NAME}\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}