{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Reasoning LLM with GRPO (R1-Zero Replication)\n",
    "\n",
    "This notebook trains Qwen2.5-1.5B using pure RL (GRPO) to develop reasoning capabilities,\n",
    "replicating the DeepSeek-R1-Zero experiment at small scale.\n",
    "\n",
    "**Requirements:** A100 GPU (40GB or 80GB). The config auto-adjusts for GPU memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers trl accelerate datasets math-verify wandb tensorboard pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repo (or upload files)\n",
    "import os\n",
    "REPO_URL = \"\"  # Set your repo URL here if using git\n",
    "PROJECT_DIR = \"/content/reson-llm\"\n",
    "\n",
    "if REPO_URL and not os.path.exists(PROJECT_DIR):\n",
    "    !git clone {REPO_URL} {PROJECT_DIR}\n",
    "elif not os.path.exists(PROJECT_DIR):\n",
    "    print(\"Please upload the reson-llm project files or set REPO_URL\")\n",
    "\n",
    "os.chdir(PROJECT_DIR)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU check and memory-based config selection\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"No GPU detected! This notebook requires an A100 GPU.\")\n",
    "\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "gpu_mem_gb = torch.cuda.get_device_properties(0).total_mem / 1e9\n",
    "print(f\"GPU: {gpu_name}\")\n",
    "print(f\"Memory: {gpu_mem_gb:.1f} GB\")\n",
    "\n",
    "# Auto-select config based on GPU memory\n",
    "if gpu_mem_gb >= 70:\n",
    "    GPU_TIER = \"80gb\"\n",
    "    print(\"Config: Full (G=16, max_completion=2048)\")\n",
    "elif gpu_mem_gb >= 35:\n",
    "    GPU_TIER = \"40gb\"\n",
    "    print(\"Config: Reduced (G=8, max_completion=1024)\")\n",
    "else:\n",
    "    GPU_TIER = \"small\"\n",
    "    print(\"WARNING: Limited GPU memory. Using minimal config.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B login\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive mount for checkpoint persistence\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "DRIVE_CHECKPOINT_DIR = \"/content/drive/MyDrive/reson-llm-checkpoints\"\n",
    "os.makedirs(DRIVE_CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"Checkpoints will be saved to: {DRIVE_CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load base config\n",
    "with open(\"configs/grpo_qwen2.5_1.5b.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Override output_dir to use Google Drive\n",
    "config[\"output_dir\"] = DRIVE_CHECKPOINT_DIR\n",
    "\n",
    "# Adjust for GPU tier\n",
    "if GPU_TIER == \"40gb\":\n",
    "    config[\"num_generations\"] = 8\n",
    "    config[\"max_completion_length\"] = 1024\n",
    "    config[\"per_device_train_batch_size\"] = 1\n",
    "    print(\"Adjusted config for 40GB GPU\")\n",
    "elif GPU_TIER == \"small\":\n",
    "    config[\"num_generations\"] = 4\n",
    "    config[\"max_completion_length\"] = 512\n",
    "    config[\"per_device_train_batch_size\"] = 1\n",
    "    config[\"gradient_accumulation_steps\"] = 4\n",
    "    print(\"Adjusted config for small GPU\")\n",
    "\n",
    "# Write adjusted config\n",
    "RUNTIME_CONFIG = \"/content/runtime_config.yaml\"\n",
    "with open(RUNTIME_CONFIG, \"w\") as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(\"\\nTraining config:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing checkpoint to resume from\n",
    "import glob\n",
    "\n",
    "checkpoints = sorted(glob.glob(f\"{DRIVE_CHECKPOINT_DIR}/checkpoint-*\"))\n",
    "resume_arg = \"\"\n",
    "if checkpoints:\n",
    "    latest = checkpoints[-1]\n",
    "    print(f\"Found checkpoint: {latest}\")\n",
    "    resume_arg = f\"--resume_from_checkpoint {latest}\"\n",
    "else:\n",
    "    print(\"No checkpoint found, starting fresh.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch training\n",
    "!python src/train_grpo.py --config {RUNTIME_CONFIG} {resume_arg}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monitor (TensorBoard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {DRIVE_CHECKPOINT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick eval on 200 samples\n",
    "!python src/evaluate.py \\\n",
    "    --model_path {DRIVE_CHECKPOINT_DIR} \\\n",
    "    --num_samples 200 \\\n",
    "    --output_dir eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline comparison (raw Qwen2.5-1.5B)\n",
    "!python src/evaluate.py \\\n",
    "    --model_path Qwen/Qwen2.5-1.5B \\\n",
    "    --num_samples 200 \\\n",
    "    --output_dir eval_results_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "import json\n",
    "\n",
    "with open(\"eval_results/summary.json\") as f:\n",
    "    trained = json.load(f)\n",
    "with open(\"eval_results_baseline/summary.json\") as f:\n",
    "    baseline = json.load(f)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<30} {'Baseline':>12} {'Trained':>12}\")\n",
    "print(\"=\" * 60)\n",
    "for dataset in [\"gsm8k\", \"math\"]:\n",
    "    if dataset in trained and dataset in baseline:\n",
    "        b = baseline[dataset]\n",
    "        t = trained[dataset]\n",
    "        print(f\"{dataset.upper()} accuracy:{'':>13} {b['accuracy']:>11.1%} {t['accuracy']:>11.1%}\")\n",
    "        print(f\"{dataset.upper()} format compliance:{'':>4} {b['format_compliance']:>11.1%} {t['format_compliance']:>11.1%}\")\n",
    "        print(f\"{dataset.upper()} avg think tokens:{'':>5} {b['avg_think_tokens']:>11.0f} {t['avg_think_tokens']:>11.0f}\")\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Push to Hub (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and set your HF username to push\n",
    "# from huggingface_hub import login\n",
    "# login()\n",
    "\n",
    "# HF_USERNAME = \"your-username\"\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# model = AutoModelForCausalLM.from_pretrained(DRIVE_CHECKPOINT_DIR, torch_dtype=torch.bfloat16)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(DRIVE_CHECKPOINT_DIR)\n",
    "# model.push_to_hub(f\"{HF_USERNAME}/qwen2.5-1.5b-r1zero-grpo\")\n",
    "# tokenizer.push_to_hub(f\"{HF_USERNAME}/qwen2.5-1.5b-r1zero-grpo\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
