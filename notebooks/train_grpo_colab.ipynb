{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Reasoning LLM with GRPO (R1-Zero Replication)\n",
    "\n",
    "This notebook trains **Qwen2.5-3B** using pure RL (GRPO) to develop reasoning capabilities,\n",
    "replicating the DeepSeek-R1-Zero experiment.\n",
    "\n",
    "**Supported setups** (auto-detected):\n",
    "| Setup | Config | Time | Cost |\n",
    "|-------|--------|------|------|\n",
    "| 1× H200 141GB | 3B, G=16, batch=2 | ~5h | — |\n",
    "| 3× A100 80GB (Jarvis Labs) | 3B, G=12, data-parallel | ~4.3h | ~$17 |\n",
    "| 2× A100 80GB | 3B, G=12, data-parallel | ~5.5h | ~$14 |\n",
    "| 1× H100 80GB | 3B, G=12 | ~6h | ~$18 |\n",
    "| 1× A100 80GB | 1.5B, G=16 | ~8h | ~$10 |\n",
    "| 1× A100 40GB | 1.5B, G=8 | ~5.5h | ~$7 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (TRL >= 0.16 requires PyTorch >= 2.6 for FSDPModule)\n",
    "!pip install -q -U torch torchvision torchaudio\n",
    "!pip install -q transformers trl accelerate datasets math-verify wandb tensorboard pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platform + clone repo\n",
    "import os\n",
    "\n",
    "PLATFORM = \"jarvis\"  # Change to \"colab\" if running on Google Colab\n",
    "REPO_URL = \"https://github.com/manojkgorle/smol-reason\"\n",
    "\n",
    "if PLATFORM == \"colab\":\n",
    "    PROJECT_DIR = \"/content/smol-reason\"\n",
    "else:\n",
    "    # Jarvis Labs — use home directory\n",
    "    PROJECT_DIR = os.path.expanduser(\"~/smol-reason\")\n",
    "\n",
    "if REPO_URL and not os.path.exists(PROJECT_DIR):\n",
    "    !git clone {REPO_URL} {PROJECT_DIR}\n",
    "elif not os.path.exists(PROJECT_DIR):\n",
    "    print(\"Please upload the reson-llm project files or set REPO_URL\")\n",
    "\n",
    "os.chdir(PROJECT_DIR)\n",
    "print(f\"Platform: {PLATFORM}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU check — detect count, type, and memory to auto-select config\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"No GPU detected! This notebook requires a GPU.\")\n",
    "\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "\n",
    "print(f\"GPUs found: {NUM_GPUS}× {gpu_name}\")\n",
    "print(f\"Memory per GPU: {gpu_mem_gb:.1f} GB\")\n",
    "print(f\"Total GPU memory: {NUM_GPUS * gpu_mem_gb:.1f} GB\")\n",
    "\n",
    "# Auto-select config:\n",
    "#   1× H200 141GB → 3B model, G=16, batch=2 (best single-GPU config)\n",
    "#   3× A100 80GB  → 3B model, data-parallel\n",
    "#   2× A100 80GB  → 3B model, data-parallel\n",
    "#   1× H100 80GB  → 3B model, single GPU\n",
    "#   1× A100 80GB  → 1.5B model, G=16\n",
    "#   1× A100 40GB  → 1.5B model, G=8\n",
    "#   Other         → 1.5B model, minimal\n",
    "is_h200 = \"H200\" in gpu_name\n",
    "is_h100 = \"H100\" in gpu_name\n",
    "is_a100_80 = \"A100\" in gpu_name and gpu_mem_gb >= 70\n",
    "\n",
    "if is_h200 and gpu_mem_gb >= 130:\n",
    "    GPU_TIER = \"h200_141gb\"\n",
    "    CONFIG_FILE = \"configs/grpo_qwen2.5_3b_h200.yaml\"\n",
    "    print(\"\\nConfig: Qwen2.5-3B on H200 (G=16, batch=2, max_completion=2048)\")\n",
    "elif NUM_GPUS >= 3 and is_a100_80:\n",
    "    GPU_TIER = \"3xa100_80gb\"\n",
    "    CONFIG_FILE = \"configs/grpo_qwen2.5_3b_a100x3.yaml\"\n",
    "    print(f\"\\nConfig: Qwen2.5-3B on {NUM_GPUS}× A100 80GB (data-parallel, G=12)\")\n",
    "elif NUM_GPUS >= 2 and is_a100_80:\n",
    "    GPU_TIER = \"2xa100_80gb\"\n",
    "    CONFIG_FILE = \"configs/grpo_qwen2.5_3b_a100x2.yaml\"\n",
    "    print(f\"\\nConfig: Qwen2.5-3B on {NUM_GPUS}× A100 80GB (data-parallel, G=12)\")\n",
    "elif (is_h100 or is_h200) and gpu_mem_gb >= 70:\n",
    "    GPU_TIER = \"h100_80gb\"\n",
    "    CONFIG_FILE = \"configs/grpo_qwen2.5_3b_h100.yaml\"\n",
    "    print(\"\\nConfig: Qwen2.5-3B on H100 (G=12, max_completion=1536)\")\n",
    "elif gpu_mem_gb >= 70:\n",
    "    GPU_TIER = \"a100_80gb\"\n",
    "    CONFIG_FILE = \"configs/grpo_qwen2.5_1.5b.yaml\"\n",
    "    print(\"\\nConfig: Qwen2.5-1.5B full (G=16, max_completion=2048)\")\n",
    "elif gpu_mem_gb >= 35:\n",
    "    GPU_TIER = \"40gb\"\n",
    "    CONFIG_FILE = \"configs/grpo_qwen2.5_1.5b.yaml\"\n",
    "    print(\"\\nConfig: Qwen2.5-1.5B reduced (G=8, max_completion=1024)\")\n",
    "else:\n",
    "    GPU_TIER = \"small\"\n",
    "    CONFIG_FILE = \"configs/grpo_qwen2.5_1.5b.yaml\"\n",
    "    print(\"\\nWARNING: Limited GPU memory. Using minimal config.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B login\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint storage setup\n",
    "#\n",
    "# Jarvis Labs: persistent local storage — checkpoints survive session stop/restart.\n",
    "# Google Colab: mount Google Drive for persistence across session disconnects.\n",
    "\n",
    "if PLATFORM == \"colab\":\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    CHECKPOINT_DIR = \"/content/drive/MyDrive/reson-llm-checkpoints\"\n",
    "else:\n",
    "    # Jarvis Labs / generic GPU server — local storage persists\n",
    "    CHECKPOINT_DIR = \"outputs/grpo-qwen2.5-3b\"\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"Checkpoints: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load the auto-selected config\n",
    "print(f\"Loading config: {CONFIG_FILE}\")\n",
    "with open(CONFIG_FILE) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Override output_dir to use our checkpoint directory\n",
    "config[\"output_dir\"] = CHECKPOINT_DIR\n",
    "\n",
    "# Apply GPU-tier-specific overrides for smaller GPUs\n",
    "if GPU_TIER == \"40gb\":\n",
    "    config[\"num_generations\"] = 8\n",
    "    config[\"max_completion_length\"] = 1024\n",
    "    config[\"per_device_train_batch_size\"] = 1\n",
    "    print(\"Adjusted config for 40GB GPU\")\n",
    "elif GPU_TIER == \"small\":\n",
    "    config[\"num_generations\"] = 4\n",
    "    config[\"max_completion_length\"] = 512\n",
    "    config[\"per_device_train_batch_size\"] = 1\n",
    "    config[\"gradient_accumulation_steps\"] = 4\n",
    "    print(\"Adjusted config for small GPU\")\n",
    "\n",
    "# Write adjusted config\n",
    "RUNTIME_CONFIG = os.path.join(PROJECT_DIR, \"runtime_config.yaml\")\n",
    "with open(RUNTIME_CONFIG, \"w\") as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"\\nModel: {config['model_name_or_path']}\")\n",
    "print(f\"GPU tier: {GPU_TIER} ({NUM_GPUS}× GPU)\")\n",
    "print(f\"\\nTraining config:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing checkpoint to resume from\n",
    "import glob\n",
    "\n",
    "checkpoints = sorted(glob.glob(f\"{CHECKPOINT_DIR}/checkpoint-*\"))\n",
    "resume_arg = \"\"\n",
    "if checkpoints:\n",
    "    latest = checkpoints[-1]\n",
    "    print(f\"Found checkpoint: {latest}\")\n",
    "    resume_arg = f\"--resume_from_checkpoint {latest}\"\n",
    "else:\n",
    "    print(\"No checkpoint found, starting fresh.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch training\n",
    "# Multi-GPU: use accelerate launch with data parallelism\n",
    "# Single-GPU: run directly with python\n",
    "# PYTHONPATH ensures `from src.* import` works\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = PROJECT_DIR\n",
    "\n",
    "if NUM_GPUS > 1:\n",
    "    print(f\"Launching distributed training on {NUM_GPUS} GPUs...\")\n",
    "    !PYTHONPATH={PROJECT_DIR} accelerate launch --num_processes {NUM_GPUS} src/train_grpo.py \\\n",
    "        --config {RUNTIME_CONFIG} {resume_arg}\n",
    "else:\n",
    "    print(\"Launching single-GPU training...\")\n",
    "    !PYTHONPATH={PROJECT_DIR} python src/train_grpo.py --config {RUNTIME_CONFIG} {resume_arg}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monitor (TensorBoard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {CHECKPOINT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick eval on 200 samples\n",
    "!PYTHONPATH={PROJECT_DIR} python src/evaluate.py \\\n",
    "    --model_path {CHECKPOINT_DIR} \\\n",
    "    --num_samples 200 \\\n",
    "    --output_dir eval_results\n",
    "\n",
    "print(f\"\\nModel evaluated: {config['model_name_or_path']} (trained)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline comparison (raw base model, before RL training)\n",
    "BASE_MODEL = config[\"model_name_or_path\"]\n",
    "print(f\"Evaluating baseline: {BASE_MODEL}\")\n",
    "!PYTHONPATH={PROJECT_DIR} python src/evaluate.py \\\n",
    "    --model_path {BASE_MODEL} \\\n",
    "    --num_samples 200 \\\n",
    "    --output_dir eval_results_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "import json\n",
    "\n",
    "with open(\"eval_results/summary.json\") as f:\n",
    "    trained = json.load(f)\n",
    "with open(\"eval_results_baseline/summary.json\") as f:\n",
    "    baseline = json.load(f)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<30} {'Baseline':>12} {'Trained':>12}\")\n",
    "print(\"=\" * 60)\n",
    "for dataset in [\"gsm8k\", \"math\"]:\n",
    "    if dataset in trained and dataset in baseline:\n",
    "        b = baseline[dataset]\n",
    "        t = trained[dataset]\n",
    "        print(f\"{dataset.upper()} accuracy:{'':>13} {b['accuracy']:>11.1%} {t['accuracy']:>11.1%}\")\n",
    "        print(f\"{dataset.upper()} format compliance:{'':>4} {b['format_compliance']:>11.1%} {t['format_compliance']:>11.1%}\")\n",
    "        print(f\"{dataset.upper()} avg think tokens:{'':>5} {b['avg_think_tokens']:>11.0f} {t['avg_think_tokens']:>11.0f}\")\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Push to Hub (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and set your HF username to push\n",
    "# from huggingface_hub import login\n",
    "# login()\n",
    "\n",
    "# HF_USERNAME = \"your-username\"\n",
    "# MODEL_TAG = config[\"model_name_or_path\"].split(\"/\")[-1].lower()\n",
    "# REPO_NAME = f\"{HF_USERNAME}/{MODEL_TAG}-r1zero-grpo\"\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# model = AutoModelForCausalLM.from_pretrained(CHECKPOINT_DIR, torch_dtype=torch.bfloat16)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_DIR)\n",
    "# model.push_to_hub(REPO_NAME)\n",
    "# tokenizer.push_to_hub(REPO_NAME)\n",
    "# print(f\"Pushed to: https://huggingface.co/{REPO_NAME}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
