# =============================================================================
# GRPO Training Config — Qwen2.5-3B on 2× A100 80GB (Data Parallel)
# =============================================================================
#
# Multi-GPU config using 2 A100 80GB GPUs in a single node.
# Each GPU holds a full copy of the 3B model and processes different prompts.
# Gradients are synced via all-reduce before each optimizer step.
#
# Launch with:
#   accelerate launch --num_processes 2 src/train_grpo.py \
#       --config configs/grpo_qwen2.5_3b_a100x2.yaml
#
# Or:
#   torchrun --nproc_per_node 2 src/train_grpo.py \
#       --config configs/grpo_qwen2.5_3b_a100x2.yaml
#
# Memory per GPU:  ~57 GB (same as single-GPU — full model replica)
# Expected time:   ~5.5h (1000 steps)
# Expected cost:   ~$14 on Jarvis Labs (2 × $1.29/hr × 5.5h)
#
# === How multi-GPU GRPO works ===
#
#   GPU 0: 1 prompt × 4 accum steps × 12 completions = 48 completions
#   GPU 1: 1 prompt × 4 accum steps × 12 completions = 48 completions
#                                                        ─────────────
#   Total per update:                                    96 completions
#   Unique prompts per update:                           8
#
# Gradients are averaged across both GPUs (all-reduce) before
# the optimizer step. This means both model copies stay in sync.
#
# The key tuning is gradient_accumulation_steps: reduced from 8 (single GPU)
# to 4, because 2 GPUs × 4 accum = single GPU × 8 accum.
# =============================================================================

# --- Model ---
model_name_or_path: Qwen/Qwen2.5-3B
torch_dtype: bfloat16
model_init_kwargs:
  torch_dtype: bfloat16
  trust_remote_code: true

# --- GRPO Algorithm ---
num_generations: 12
beta: 0.0
epsilon: 0.2
loss_type: dr_grpo
scale_rewards: false
mask_truncated_completions: true
reward_weights:
  - 1.0
  - 0.2

# --- Generation ---
temperature: 0.7
max_prompt_length: 512
max_completion_length: 1536

# --- Training ---
learning_rate: 1.0e-6

# Each GPU processes 1 prompt per micro-batch (× 12 generations = 12 completions).
per_device_train_batch_size: 1

# Reduced from 8 to 4 because we have 2 GPUs.
# Effective prompts per update: 2 GPUs × 1 batch × 4 accum = 8
# (matches single-GPU target exactly)
gradient_accumulation_steps: 4

max_steps: 1000
warmup_ratio: 0.03
max_grad_norm: 0.2

# --- Memory Optimization ---
gradient_checkpointing: true
bf16: true

# --- Logging ---
logging_steps: 1
log_completions: true
report_to:
  - wandb
  - tensorboard
run_name: grpo-qwen2.5-3b-r1zero-2xa100

# --- Checkpointing ---
output_dir: outputs/grpo-qwen2.5-3b
save_steps: 100
save_total_limit: 3

eval_strategy: "no"
